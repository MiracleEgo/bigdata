# 常见数据库引擎

* MyISAM
* InnoDB
* ISAM
  * 读取速度快，不占用大量内存和存储资源
  * 不运行事务，不容错，需经常备份实时数据
* MEMORY
  * 每个表可有32个索引，每个索引16列
  * 表中数据存储到内存中
* Archive
  * 为大量很少引用的历史，归档，或安全审计信息的存储和检索提供解决方案

# 如何设计一个关系型数据库

* RDBMS  
  * 程序实例
    * 存储管理
    * 缓存机制
    * SQL解析
    * 日志管理
    * 权限划分
    * 容灾机制
    * **索引管理**
    * **锁管理**
  * 存储（文件系统）
* 阶段
  * 需求分析阶段
    * 分析用户需求 
  * 概念设计阶段
    * 设计E-R图形

  * 逻辑设计阶段

      * 设计表格 
  * 物理设计阶段
    * 设计数据库的存储方式和存储路径 
  * 实现阶段 
  * 实施维护阶段

# 索引模块

## 索引的优点

* 避免全表扫描查找数据，提升查找效率
* 创建唯一性索引，保证数据库表中每行数据的唯一性
* 使用分组和排序子句进行数据检索时，减少查询中分组和排序时间
* 可在查询中使用优化隐藏器，提高系统性能

## 索引的缺点

* **创建**维护索引需要时间
* 索引占用物理**空间**，聚簇索引占用空间更大
* 当对表中数据增删改时，需**维护**索引

## 什么样的信息能成为索引？

* 主键，唯一键，普通键等让数据具备**一定区分性的字段**
* 什么情况下适合建立索引
  * 经常出现在关键字`order by` ,`group by `, `distinct` 后面的字段
  * union等集合操作的结果集字段
  * 经常查询的字段
  * 经常用作表连接的属性

## 索引的(类型)数据结构

* 二叉查找树
* B-Tree
  * 根节点至少包括两个孩子
  * 树中每个节点最多包含有m个孩子（m>=2）
  * 除根节点和叶节点外，其他每个节点至少有ceil(m/2)个孩子
  * 所有叶子节点都在同一层
* B+-Tree（Mysql）
  * 非叶子节点的子树指针与关键字个数相同
  * 非叶子节点的子树指针P[i],指向关键字值[K[i],K[i+1]]的子树
  * 非叶子节点仅用来索引，叶子节点存储数据
  * 所有叶子节点均有一个指针链接到下一个叶子节点
* Hash
  * 仅能满足“=”，“IN”，不能使用范围查询
  * 无法被用来避免数据的排序操作
  * 不能利用部分索引键查询
  * 不能避免表扫描
  * 遇到大量Hash值相等的情况后性能并不一定比B树索引高
* BitMap索引

## 密集索引和稀疏索引的区别

* 密集索引文件中的每个搜索码值都对应一个索引值
* 稀疏索引文件只为索引码的某些值建立索引项

## [聚簇索引和非聚簇索引的区别](https://blog.csdn.net/alexdamiao/article/details/51934917)

* InnoDB的B+树可能存储整行数据(聚簇索引),也可能存储主键的值(非聚簇索引)
* 聚簇索引查询只用一次,非聚簇索引需要回表查询多次,通过覆盖索引也可以只查询一次
* **覆盖索引**
  * 一个查询语句的执行只用从索引中就能够取得,不必从数据表中读取,称为实现了索引的覆盖,避免查到索引后回表操作,减少IO提高效率

## 回表

* 我们有个主键为ID的索引，和一个普通name字段的索引，我们在普通字段上搜索：

  `select * from table where name = '丙丙'`

  执行的流程是先查询到name索引上的“丙丙”，然后找到他的id是2，最后去主键索引，找到id为2对应的值。

  回到主键索引树搜索的过程，就是**回表**。不过也有方法避免回表，那就是**覆盖索引**。

## [覆盖索引](https://www.cnblogs.com/chenpingzhao/p/4776981.html)

* 刚才我们是 select * ，查询所有的，我们如果只查询ID那，其实在Name字段的索引上就已经有了，那就不需要回表了。
* 覆盖索引可以减少树的搜索次数，提升性能，他也是我们在实际开发过程中经常用来优化查询效率的手段。
* 很多**联合索引**的建立，就是为了支持覆盖索引，特定的业务能极大的提升效率

## 联合索引

### 最左匹配原则

- mysql索引规则中要求复合索引要想使用第二个索引，必须先使用第一个索引。（而且第一个索引必须是等值匹配）。
- **列的排列顺序决定了可命中索引的列数**
- mysql会一直向右匹配直到遇到范围查询（>,<,between,like）就停止匹配
  - 比如a=3 and b=4 and c>5 and d=6 
  - 如果建立（a,b,c,d）顺序的索引，d是用不到索引的
  - 如果建立（a,b,d,c）的索引，则都可以用到，a,b,d的顺序可以任意调整。
  - =和in可以乱序，比如a=1 and b =2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会优化成索引可以识别的形式
- 当创建一个联合索引时,如(key1,key2,key3),相当于他建了(key1),(key1,key2),(key1,key2.key3)

### 索引失效

- 若条件中有or,即使其中有条件带索引也不会使用(尽量少使用or的原因)
- 对于多列索引,不是使用的第一部分,则不会使用索引
- like查询是以%开头
- 如果列类型是字符串,要在条件中使用绰号引起来,否则不会使用索引
- 如果MySQL估计使用全表扫描比索引快,则不使用索引

## 定位并优化慢查询Sql

* 根据慢日志定位慢查询sql
* 修改sql或者尽量让sql走索引
* 使用explain等工具分析sql
  * `type`表中找到所需行的方式
    * `ALL` `index` `range` `ref` `eq_ref` `const` `system` `NULL`
  * `key` 查询中实际使用的索引，若没有使用索引则为NULL
  * `ref` 表的连接匹配条件，即哪些列或常量被用于查找索引列上的值
  * `table` select查询的表的名字

## 查询优化器

* 一条SQL的查询可有不同执行方案,需通过优化器进行选择成本最低方案
  * 根据搜索条件,找出最有可能使用的索引
  * 计算全表扫描的代价
  * 计算使用不同索引执行查询的代价
  * 对比各种方案的代价,找出成本最低的



## 为何性别不适合用索引

* 访问索引需额外IO开销,从索引中拿到的只是地址,想访问真正的数据还需对表进行IO,此时开销并不一定比直接对表扫描小

# 锁模块

## MyISAM与InnoDB关于锁方面的区别是什么？

* MyISAM默认用的是表级锁，不支持行级锁
* InnoDB默认用的是行级锁，也支持表级锁
  * 走索引时，用行锁
  * 不走索引时，用表锁

## MyISAM适合的场景

* 频繁执行全表count
* 对数据进行增删改的频率不高，查询非常频繁
* 没有事务

## InnoDB适合的场景

* 数据增删改查频繁
* 可靠性要求比较高，要求支持事务

## [锁的分类](https://zhuanlan.zhihu.com/p/52678870)

![img](https://pic3.zhimg.com/80/v2-5cf8b96fdca1428e6f3cce863fdfa73e_720w.jpg)

### 乐观锁

* 通常乐观锁通过使用版本号/时间戳实现

  `update table set fields = #{fields},version=#{new_version} where id=#{id} and version = #{old_version}`

* 乐观锁与悲观锁不同的是，它是一种逻辑上的锁，而不需要数据库提供锁机制来支持

## [行锁种类](https://www.cnblogs.com/zhoujinyi/p/3435982.html)

* 行锁就是一锁锁一行或者多行记录，mysql的**行锁是基于索引加载的**，所以行锁是要加在索引响应的行上，即命中索引

### [记录锁](https://zhuanlan.zhihu.com/p/52678870)

* 记录锁锁的是表中的某一条记录，记录锁的出现条件<span style="color:red">**必须是精准命中索引并且索引是唯一索引**</span>，如主键id，就像我们上面描述行锁时使用的sql语句图，在这里就挺适用的。

### [GAP锁(间隙锁)](https://zhuanlan.zhihu.com/p/52678870)

* **防止幻读,在事务读取数据间隙上锁,防止在这个时间内其他事务修改数据**
* 间隙锁的触发条件必然是命中索引的，<span style="color:red">当我们查询数据用范围查询而不是相等条件查询时，**查询条件命中索引**，并且**没有查询到符合条件的记录**，此时就会将查询条件中的范围数据进行锁定(即使是范围库中不存在的数据也会被锁定)</span>
* **间隙锁只会出现在可重复读的事务隔离级别中**
* 根据检索条件向左寻找最靠近检索条件的记录A作为左区间，向右寻找最靠近检索条件的记录值B作为右区间，即锁定的间隙为（A，B]
  * 防止间隙内有新数据被插入
  * 防止已存在的数据更新成间隙内的数

### [next-key锁（临键锁）](https://www.cnblogs.com/zhoujinyi/p/3435982.html)

* 1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。

* 间隙锁的触发条件是命中索引，范围查询没有匹配到相关记录。而临键锁恰好相反，临键锁的触发条件也是**查询条件命中索引**，不过，临键锁**有匹配到数据库记录**；

* 因为**InnoDB对于行的查询都是采用了Next-Key Lock的算法**，锁定的不是单个值，而是一个范围，按照这个方法是会和第一次测试结果一样。但是，**当查询的索引含有唯一属性的时候，Next-Key Lock 会进行优化，将其降级为Record Lock，即仅锁住索引本身，不是范围**。

  注意：通过主键或则唯一索引来锁定不存在的值，也会产生GAP锁定。

* gap锁与行锁的组合，<span style='color:red'>InnoDB中，更新非唯一索引对应的记录时会加上Next-Key锁，如果更新记录为空则只能加gap锁</span>

## [如何实现分布式锁](https://blog.csdn.net/weixin_33805152/article/details/93633536)

* 利用主键唯一的特性，如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，当方法执行完毕之后，想要释放锁的话，删除这条数据库记录即可。

* **创建task_lock表，注意key作为唯一主键**

* ![表结构](https://img-blog.csdnimg.cn/20190625172056420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zMzgwNTE1Mg==,size_16,color_FFFFFF,t_70)

  ```java
  //实现原理是:遍历表中的所有记录.筛选出过期的key,然后删除
  //每个小时第52分钟执行一次
      @Scheduled(cron = "0 52 * * * ?")
      private void checkSynTaskKeyIsExpire() {
          logger.info("=========开始检查数据库分布式锁的过期时间=======");
          List<TaskLock> taskLocks = taskLockMapper.findAll();
          //筛选出过期的key
          List<TaskLock> taskLockList = taskLocks.stream().
                  filter(taskLock -> new Date().getTime() - taskLock.getUtime().getTime()
                          - taskLock.getTimeout() * 1000 > 0).collect(Collectors.toList());
          taskLockList.forEach(taskLock -> {
              taskLockMapper.deleteByPrimaryKey(taskLock.getKey());
          });
          logger.info("=========trs_task_lock 删除{}条 过期的key=======",taskLockList.size());
      }
  
  ```

  

# 事务

## 为何要有事务

* 保证数据的最终一致性

## [事务的特性](https://blog.csdn.net/l1394049664/article/details/81814090)

* 原子性
  * 事务包含的操作要么全部成功,要么全部失败回滚,如果成功则必须完全应用到数据库,如果失败,则不能对数据库有任何影响
* 一致性
  * 事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态
  * 拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。
* 持久性
  * 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。
* 隔离性
  * 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。
  * 对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。

## 如何保证事务的隔离性

* 加锁
* **频繁的加锁会带来什么问题**
  * 读数据的时候没办法修改。修改数据的时候没办法读取，极大的降低了数据库性能。
* **数据库是如何解决加锁后的性能问题的**
  * MVCC 多版本控制实现读取数据不用加锁， 可以让读取数据同时修改。修改数据时同时可读取。

## 事务隔离级别

* 可序列化
  * 最高隔离级别,强制事务串行执行,避免了幻读问题
  * 可序列化在读取的每一行数据上加锁,可能导致大量超时和锁争用问题
* 可重复读（默认）
  * **解决了脏读,不可重复读的发生**,保证多次读取同样记录结果是一样的,但无法解决幻读
  * 幻读:当某事务在读取某个范围内记录时,另一个事务又在该范围内插入了新的记录,当之前的事务再次读取该范围记录时,会产生幻行,通过多版本并发控制(MVCC)解决
* 提交读
  * 大多数数据库是默认提交读,MySQL不是
  * 一个事务从开始直到提交前,所做的任何修改对其他事务不可见,有时也叫不可重复读,因为两次查询的结果可能不同
  * **可避免脏读的发生**
* 未提交读
  * **事务可以读取未提交的数据(脏读**),实际中很少使用,任何情况都无法保证

## [事务隔离级别引起的问题](https://www.cnblogs.com/fjdingsd/p/5273008.html)

* 更新丢失
  
  * 两个事务对同一数据进行修改，后修改的覆盖先修改的
  
* 脏读
  
  * 事务1中修改数据未提交,被其他事务读到,然后事务1回滚,此时读的数据为脏数据
  
* 不可重复读
  
  * 指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了
  * 例如事务T1在读取某一数据，而事务T2立马修改了这个数据并且提交事务给数据库，事务T1再次读取该数据就得到了不同的结果，发送了不可重复读。
  
* 幻读
  
  * 第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。 同时，第二个事务也修改这个表中的数据，这种修改是向表中**插入一行新数据**。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象 发生了幻觉一样。
  
* **幻读和不可重复读都是读取了另一条已经提交的事务**（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。

* 关键语法

  * GROUP BY
    * 查询所有同学的学号，选课数，总成绩

  ```SQL
  select student_id,count(course_id),sum(score) 
  from score 
  group by student_id 
  ```

  * 查询所有同学的学号，姓名，选课数，总成绩

  ```sql
    select s.student_id,stu.name,count(s.course_id),sum(s.score) 
    from 
    	score s ,
    	student stu
    where s.student_id = stu.student_id
    group by s.student_id; 
  ```

* HAVING

  * 查询平均成绩大于60分的同学的学号和平均成绩

  ```sql
  select student_id,avg(score)
  from score
  group by student_id
  having avg(score)>60;
  ```

* 统计相关

  * COUNT
  * SUM
  * MAX
  * MIN
  * AVG

查询没有学全所有课的同学的学号，姓名

```sql
select stu.student_id,stu.name
from 
	student stu,
	score s
where stu.student_id = s.student_id
group by student_id
having count(*) <
(
    select count(*) from course
)
```

# 数据库范式

* 1NF
  * 每个属性都不可再分
* 2NF
  * 属性完全依赖于主键（消除部分子函数依赖）
* 3NF
  * 属性不依赖于其它非主属性（消除传递依赖）
* BCNF
  * 在1NF基础上，任何非主属性不能对主键子集依赖（在3NF基础上消除对主码子集的依赖）
* 4NF
  * 要求把同一表内的多对多关系删除
* 5NF
  * 从最终结构重新建立原始结构

# 数据库连接池原理

* 连接池的建立
  * 系统初始化时根据系统配置建立连接池，并在池中建立几个连接对象
  * Java中可使用Vector,Stack建立连接池
* 连接池管理
  * 客户请求数据库连接
    * 查看连接池中是否有空闲连接，若有则分配给客户；若无，则查看当前连接是否到最大连接数，若否则新建连接给客户；若是则按设定最大等待时间等待，若走出最大等待时间，则抛异常给客户
  * 当客户释放连接时
    * 判断连接引用次数是否超过规定值，若超过则从连接池中删除，否则保留
* 连接池关闭
  * 程序退出时，关闭连接池中所有连接，释放连接池相关资源

# 分库分表

* 概念
  * 分库
    * 用户id直接mod分成库的数目大小，将大库分成小库
  * 分表
    * 用户 id 直接 mod 分成表的数目大小， 将大表拆成小表
  * 分库分表
    * 方式1
      * 中间变量=`user_id %(分库数量*每个库的表数量)`
      * `库 = 取整数（中间变量/每个库的表数量）`
      * `表 = 中间变量 % 每个库的表数量`
    * 垂直切分
      * 单机的ACID被打破，数据到多机后，原来单机通过事务来进行处理逻辑会受很大影响
      * Join操作困难，因数据可能在两个数据库中了，不能方便利用数据库自身join
      * 靠外键进行约束的场景会受到影响
    * 水平切分
      * ACID被打破
      * Join操作受影响
      * 靠外键约束场景受影响
      * 依赖单库的自增序列生成唯一ID会受影响
      * 针对单个逻辑意义上的表的查询要跨库

## 如何解决分库分表带来的坏处

* ACID解决方法
  * 两阶段提交
    * 事务在第一阶段对资源进行准备，若在准备阶段有一个资源失败，那么在第二阶段的处理就是回滚所有资源，否则进行Commit操作
* 水平切分自增ID破坏
  * 将所有ID集中放在一个地方管理，对每个ID序列独立管理，每台机器使用ID时都从这个ID造成器上进行获取
* 跨库Join
  * 在应用层将原来数据库中Join操作分成多次数据库操作
  * 数据冗余
    * 将常用信息进行冗余，将原来需要Join的返回信息变为单表查询
  * 借助外部系统斛同跨库问题
    * 如搜索引擎
* 外键约束
  * 需要分库后的每个单库的数据是内聚的，否则就只能靠应用层的判断，容错方式

# 解决加锁后的性能问题:MVCC

* 概念

  * <span style='color:red'>为了提供更好的并发，InnoDB提供了非锁定读：不需要等待访问行上的锁释放，读取行的一个快照。</span>>该方法是通过InnoDB的一个特性：MVCC来实现的
  * 同一份数据临时保留多版本的一种方式，进面实现并发控制

* 解决的问题

  * 同时读写数据库时，读数据的人会看到不一致数据

* 实现原理

  * 通过每行记录后面保存的两个隐藏的列实现，一个保存了行的删除时间，一个保存了系统版本号

  * 开始时系统版本号会作为事务版本号

  * INSERT - InnoDB为插入的每一行保存当前系统版本号作为行版本号。

    DELETE - InnoDB为删除的每一行保存当前系统版本号作为行删除标识。

    UPDATE - InnoDB为插入一行新纪录，保存当前系统版本号作为行版本号，同时，保存当前系统版本号到原来的行作为行删除标识。

* 优点

  * **保存两个额外系统版本号，使大多读操作都可以不用加锁**，这样的设计使得读数据操作很简单

* 缺点

  * 每行记录都需要额外存储空间，需要做更多的行检查工作，以及额外的维护工作

## MVCC解决了什么问题

* MVCC的实现，是通过保存数据在某个时间点的快照来实现的。也就是说，不管需要执行多长时间，每个事务看到的数据是一致的。根据事务开始的时间不同，每个事物对同一张表，同一时刻看到的数据可能是不一样的。
* InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列来实现。这两个列，一个保存了行的创建时间，一个保存了行的过期时间（删除时间）。并且存储的并不是真实的时间值，而是系统版本号（system version number）。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。
* 保存着两个额外的系统版本号，使大多数读操作都可以不用加锁。这样设计使得读操作简单，性能强，并且保证只会读取到符合标准的行。不足之处是没行记录都需要额外的存储空间，需要做更多的检查工作，以及一些额外的维护工作。
* **MVCC只在REPEATABLE READ和READ COMMITTED两个隔离级别下工作。其他两个隔离级别都和MVCC不兼容，因为READ UNCOMMITTED总是读取最新的数据行，而不是符合当前事务版本的数据行，而SERIALIZABLE会对所有读取到的行都加锁**。
* **：简而言之就是解决了在REPEATABLE READ和READ COMMITTED两个隔离级别下读同一行和写同一行的两个事务的并发。**
* Read Committed - 一个事务读取数据时总是读这个数据最近一次被commit的版本
* Repeatable Read - 一个事务读取数据时总是读取当前事务开始之前最后一次被commit的版本（所以底层实现时需要比较当前事务和数据被commit的版本号）。

**举个简单的例子：**

1. **一个事务A（txnId=100）修改了数据X，使得X=1，并且commit了**
2. **另外一个事务B（txnId=101）开始尝试读取X，但是还X=1。但B没有提交。**
3. **第三个事务C（txnId=102）修改了数据X，使得X=2。并且提交了**
4. **事务B又一次读取了X。这时**

- **如果事务B是Read Committed。那么就读取X的最新commit的版本，也就是X=2**
- **如果事务B是Repeatable Read。那么读取的就是当前事务（txnId=101）之前X的最新版本，也就是X被txnId=100提交的版本，即X=1。**

## MVCC隔离级别

* MVCC是实现InnoDB存储引擎实现隔离级别的一种具体方式
  * 提交读
  * 可重复读

## 在读写并发过程中如何实现多版本

## 读写并发之后如何删除旧版本

# 数据库优化

## [数据库优化的几个阶段](https://www.cnblogs.com/rjzheng/p/9619855.html)

### 优化SQL和索引

(1)[用慢查询日志定位执行效率低的`SQL`语句](https://blog.csdn.net/why444216978/article/details/80447943)

* 默认情况下MySQL默认为10s才是慢查询

* 为了能够记录慢查询，我把这个慢查询的默认时间修改成1s

  ```mysql
  mysql> set long_query_time=1;
  
  mysql>show variables like ' long_query_time'
  ```


(2)用`explain`分析`SQL`的执行计划

(3)确定问题，采取相应的优化措施，建立索引啊，等

###  搭建缓存

### 读写分离

缓存也搞不定的情况下，搞主从复制，上读写分离。在应用层，区分读写请求。或者利用现成的中间件mycat或者altas等做读写分离。
需要注意的是,只要你敢说你用了主从架构，有三个问题，你要准备:

(1)主从的好处？

回答:实现数据库备份，实现数据库负载均衡，提交数据库可用性

(2)主从的原理?

回答:如图所示（图片不是自己画的，偷懒了）

![image](https://www.cnblogs.com/images/cnblogs_com/rjzheng/1281019/o_youhua1.jpg)

3)如何解决主从一致性?

回答:这个问题，我不建议在数据库层面解决该问题。根据CAP定理，主从架构本来就是一种高可用架构，是无法满足一致性的
哪怕你采用同步复制模式或者半同步复制模式，都是弱一致性，并不是强一致性。所以，推荐还是利用缓存，来解决该问题。

步骤如下:

1、自己通过测试，计算主从延迟时间，建议mysql版本为5.7以后，因为mysql自5.7开始，多线程复制功能比较完善，一般能保证延迟在1s内。不过话说回来，mysql现在都出到8.x了，还有人用5.x的版本么。

2、数据库的写操作，先写数据库，再写cache，但是有效期很短，就比主从延时的时间稍微长一点。

3、读请求的时候，先读缓存，缓存不存在(这时主从同步已经完成)，再读数据库。

### 垂直拆分

上面四个阶段都没搞定，就来垂直拆分了。垂直拆分的复杂度还是比水平拆分小的。将你的表，按模块拆分为不同的小表。大家应该都看过《大型网站架构演变之路》，这种类型的文章或者书籍，基本都有提到这一阶段。
如果你有幸能够在什么运营商、银行等公司上班，你会发现他们一个表，几百个字段都是很常见的事情。所以，应该要进行拆分，拆分原则一般是如下三点:

(1)把不常用的字段单独放在一张表。

(2)把常用的字段单独放一张表

(3)经常组合查询的列放在一张表中（联合索引）。

## [谈谈SQL慢查询的解决思路](https://juejin.im/post/5982b6496fb9a03c476d6d1d)

### 慢SQL的系统表现

**1，数据库CPU负载高。**一般是查询语句中有很多计算逻辑，导致数据库cpu负载。

**2，IO负载高导致服务器卡住。**这个一般和全表查询没索引有关系。

**3，查询语句正常，索引正常但是还是慢。**如果表面上索引正常，但是查询慢，需要看看是否索引没有生效。

### 开启SQL慢查询的日志

要开启日志，需要在 MySQL 的配置文件 my.cnf 的 [mysqld] 项下配置慢查询日志开启，如下所示：

```bash
[mysqld]slow_query_log=1
slow_query_log_file=/var/log/mysql/log-slow-queries.log
long_query_time=2
```



# [SQL四种语言](https://www.cnblogs.com/henryhappier/archive/2010/07/05/1771295.html)

* DDL **数据库定义语言**

  * DDL不需要commit

    ```
    CREATE
    ALTER
    DROP
    TRUNCATE
    COMMENT
    RENAME
    ```

* DML **数据操纵语言**

  * 由DBMS提供，用于让用户或程序员使用，实现对数据库中数据的操作。

    ```
    SELECT
    INSERT
    UPDATE
    DELETE
    MERGE
    CALL
    EXPLAIN PLAN
    LOCK TABLE
    ```

* DCL **数据库控制语言**

  * GRANT 授权
    REVOKE 取消授权

* TCL **事务控制语言**

  * SAVEPOINT 设置保存点
    ROLLBACK  回滚
    SET TRANSACTION

# [MySQL原子性如何保证](https://juejin.im/post/5e6599b1f265da572017fbc3)

## bin log

* `binlog`记录了数据库表结构和表数据变更，比如`update/delete/insert/truncate/create`。它不会记录`select`（因为这没有对表没有进行变更）
* `binlog`无论MySQL用什么引擎，都会有的。

## redo log

* `redo log`是MySQL的InnoDB引擎所产生的。

### bin log和redo log的区别

* 存储的内容

  `binlog`记载的是`update/delete/insert`这样的SQL语句，而`redo log`记载的是物理修改的内容（xxxx页修改了xxx）。

* 功能

  * `redo log`的作用是为**持久化**而生的。写完内存，如果数据库挂了，那我们可以通过`redo log`来恢复内存还没来得及刷到磁盘的数据，将`redo log`加载到内存里边，那内存就能恢复到挂掉之前的数据了。

  * `binlog`的作用是复制和恢复而生的。

    - 主从服务器需要保持数据的一致性，通过`binlog`来同步数据。
    - 如果整个数据库的数据都被删除了，`binlog`存储着所有的数据变更情况，那么可以通过`binlog`来对数据进行恢复。

  * `redo log`**事务开始**的时候，就开始记录每次的变更信息，而`binlog`是在**事务提交**的时候才记录。
    * 如果写`redo log`失败了，那我们就认为这次事务有问题，回滚，不再写`binlog`。
    * 如果写`redo log`成功了，写`binlog`，写`binlog`写一半了，但失败了怎么办？我们还是会对这次的**事务回滚**，将无效的`binlog`给删除（因为`binlog`会影响从库的数据，所以需要做删除操作）
    * 如果写`redo log`和`binlog`都成功了，那这次算是事务才会真正成功。

## undo log

* `undo log`主要有两个作用：回滚和多版本控制(MVCC)
* `undo log`主要存储的也是逻辑日志，比如我们要`insert`一条数据了，那`undo log`会记录的一条对应的`delete`日志。我们要`update`一条记录时，它会记录一条对应**相反**的update记录。

* 因为`undo log`存储着修改之前的数据，相当于一个**前版本**，MVCC实现的是读写不阻塞，读的时候只要返回前一个版本的数据就行了。

# 主从同步

## [实现MySQL数据库的实时备份](https://www.cnblogs.com/wu-jian/p/9396739.html)

1. Master中的所有数据库变更事件写入Binary Log文件
2. 当在Slave中执行“SLAVE START”命令时，开启Slave I/O Thread，并连接Master
3. Master侦测到Slave I/O Thread的连接，开启Log Jump Thread进行响应
4. Master Binary Log经Master Log Jump Thread和Slave I/O Thread传输至Slave Relay Log 
5. Slave SQL Thread将Relay Log还原至数据，同步完成

注：可使用“SHOW PROCESSLIST”命令在Master和Slave中查看对应线程的运行情况

## [MySQL 主从同步延时问题](https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/mysql-read-write-separation.md)

### **半同步复制**

* 用来解决主库数据丢失问题
* 主库写入 binlog 日志之后，就会将**强制**此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到**至少一个从库**的 ack 之后才会认为写操作完成了。

### **并行复制**

* 用来解决主从同步延时问题。
* 指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后**并行重放不同库的日志**，这是库级别的并行。

## [MySQL出现主从同步延迟可能有哪些原因](https://blog.csdn.net/yuki5233/article/details/75909858)

1.从库太多导致复制延迟
优化：建议从库数量3-5个为宜。

2.从库硬件比主库硬件差
优化：提升硬件性能。

3.慢SQL语句过多
优化：SQL语句执行时间太长，需要优化SQL语句。

4.主从复制的设计问题
优化：主从复制单线程，可以通过多线程IO方案解决；另外MySQL5.6.3支持多线程IO复制。

5.主从库之间的网络延迟
优化：尽量链路短，提升端口带宽。

6.主库读写压力大
优化：前端加buffer和缓存。主从延迟不同步： 不管有多延迟，只要不影响业务就没事。

7.业务设计缺陷导致延迟影响业务
优化：从库没有数据改读主库。