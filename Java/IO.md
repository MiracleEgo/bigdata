# [常见I/O模型对比](https://tech.meituan.com/2016/11/04/nio.html)

* **所有的系统I/O都分为两个阶段：等待就绪和操作**。举例来说，读函数，分为等待系统可读和真正的读；同理，写函数分为等待网卡可以写和真正的写。

* **需要说明的是等待就绪的阻塞是不使用CPU的，是在“空等”；而真正的读写操作的阻塞是使用CPU的，真正在”干活”**，而且这个过程非常快，属于memory copy，带宽通常在1GB/s级别以上，可以理解为基本不耗时。

  ![emma_1](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2016/77752ed5.jpg)

* 传统的BIO里面`socket.read()`，如果TCP RecvBuffer里没有数据，函数会一直阻塞，直到收到数据，返回读到的数据。
* 对于NIO，如果TCP RecvBuffer有数据，就把数据从网卡读到内存，并且返回给用户；反之则直接返回0，**永远不会阻塞**。
* 最新的AIO(Async I/O)里面会更进一步：不但等待就绪是非阻塞的，就连数据从网卡到内存的过程也是异步的。
* 换句话说，BIO里用户最关心“我要读”，NIO里用户最关心”我可以读了”，在AIO模型里用户更需要关注的是“读完了”。
* NIO一个重要的特点是：**socket主要的读、写、注册和接收函数，在等待就绪阶段都是非阻塞的，真正的I/O操作是同步阻塞的**（消耗CPU但性能非常高）。

## [如何结合事件模型使用NIO同步非阻塞特性](https://tech.meituan.com/2016/11/04/nio.html)

* 回忆BIO模型，**之所以需要多线程，是因为在进行I/O操作的时候，一是没有办法知道到底能不能写、能不能读，只能”傻等”，**即使通过各种估算，算出来操作系统没有能力进行读写，**也没法在`socket.read()`和`socket.write()`函数中返回，这两个函数无法进行有效的中断。所以除了多开线程另起炉灶，没有好的办法利用CPU。**
* **NIO的读写函数可以立刻返回，这就给了我们不开线程利用CPU的最好机会**：如果一个连接不能读写（`socket.read()`返回0或者`socket.write()`返回0），我们可以把这件事记下来，**记录的方式通常是在Selector上注册标记位，然后切换到其它就绪的连接（channel）继续进行读写**。

* NIO的主要事件有几个：读就绪、写就绪、有新连接到来。
  * **我们首先需要注册当这几个事件到来的时候所对应的处理器。然后在合适的时机告诉事件选择器：我对这个事件感兴趣。**对于写操作，就是写不出去的时候对写事件感兴趣；对于读操作，就是完成连接和系统没有办法承载新读入的数据的时；对于accept，一般是服务器刚启动的时候；而对于connect，一般是connect失败需要重连或者直接异步调用connect的时候。
  * 其次，**用一个死循环选择就绪的事件，会执行系统调用**（Linux 2.6之前是select、poll，2.6之后是epoll，Windows是IOCP），**还会阻塞的等待新事件的到来**。新事件到来的时候，会在selector上注册标记位，标示可读、可写或者有连接到来。**注意，select是阻塞的，无论是通过操作系统的通知（epoll）还是不停的轮询(select，poll)，这个函数是阻塞的**。所以你可以放心大胆地在一个while(true)里面调用这个函数而不用担心CPU空转。

## [NIO是怎么解决掉线程的瓶颈并处理海量连接的](https://tech.meituan.com/2016/11/04/nio.html)

* NIO由原来的阻塞读写（占用线程）变成了单线程轮询事件，找到可以进行读写的网络描述符进行读写。除了事件的轮询是阻塞的（没有可干的事情必须要阻塞），剩余的I/O操作都是纯CPU操作，没有必要开启多线程。
* 并且由于线程的节约，连接数大的时候因为线程切换带来的问题也随之解决，进而为处理海量连接提供了可能。
* 单线程处理I/O的效率确实非常高，没有线程切换，只是拼命的读、写、选择事件。但现在的服务器，一般都是多核处理器，如果能够利用多核心进行I/O，无疑对效率会有更大的提高。

## Proactor与Reactor

* 一般情况下，I/O 复用机制需要事件分发器（event dispatcher）。 事件分发器的作用，即将那些读写事件源分发给各读写事件的处理者，就像送快递的在楼下喊: 谁谁谁的快递到了， 快来拿吧！开发人员在开始的时候需要在分发器那里注册感兴趣的事件，并提供相应的处理者（event handler)，或者是回调函数；事件分发器在适当的时候，会将请求的事件分发给这些handler或者回调函数。
* 涉及到事件分发器的两种模式称为：Reactor和Proactor
  * **Reactor模式是基于同步I/O的，而Proactor模式是和异步I/O相关的**。在Reactor模式中，**事件分发器等待某个事件或者可应用或个操作的状态发生**（比如文件描述符可读写，或者是socket可读写），**事件分发器就把这个事件传给事先注册的事件处理函数或者回调函数，由后者来做实际的读写操作**。
  * **而在Proactor模式中，事件处理者（或者代由事件分发器发起）直接发起一个异步读写操作（相当于请求），而实际的工作是由操作系统来完成的**。发起时，需要提供的参数包括用于存放读到数据的缓存区、读的数据大小或用于存放外发数据的缓存区，以及这个请求完后的回调函数等信息。**事件分发器得知了这个请求，它默默等待这个请求的完成，然后转发完成事件给相应的事件处理者或者回调**。举例来说，在Windows上事件处理者投递了一个异步IO操作（称为overlapped技术），事件分发器等IO Complete事件完成。这种异步模式的典型实现是基于操作系统底层异步API的，所以我们可称之为“系统级别”的或者“真正意义上”的异步，因为具体的读写是由操作系统代劳的。

## Selector.wakeup()

* 主要作用

  * 解除阻塞在`Selector.select()/select(long`)上的线程，立即返回。

  * **两次成功的select之间多次调用wakeup等价于一次调用**。

  * **如果当前没有阻塞在select上，则本次wakeup调用将作用于下一次select**——“记忆”作用。

* 为什么要唤醒？
  * 注册了新的channel或者事件。
  * channel关闭，取消注册。
  * 优先级更高的事件触发（如定时器事件），希望及时处理。
* 原理
  * Linux上利用pipe调用创建一个管道，Windows上则是一个loopback的tcp连接。这是因为win32的管道无法加入select的fd set，将管道或者TCP连接加入select fd set。
  * **wakeup往管道或者连接写入一个字节，阻塞的select因为有I/O事件就绪，立即返回。**可见，wakeup的调用开销不可忽视。

## [Buffer的选择](https://tech.meituan.com/2016/11/04/nio.html)

* 通常情况下，操作系统的一次写操作分为两步：

  * 1. 将数据从用户空间拷贝到系统空间。 

    2. 从系统空间往网卡写。


* 同理，读操作也分为两步： ① 将数据从网卡拷贝到系统空间； ② 将数据从系统空间拷贝到用户空间。

* 对于NIO来说，缓存的使用可以使用DirectByteBuffer和HeapByteBuffer。**如果使用了DirectByteBuffer，一般来说可以减少一次系统空间到用户空间的拷贝**。但Buffer创建和销毁的成本更高，更不宜维护，通常会用内存池来提高性能。

* 如果数据量比较小的中小应用情况下，可以考虑使用heapBuffer；反之可以用directBuffer。

* 使用NIO != 高性能，当连接数<1000，**并发程度不高或者局域网环境下NIO并没有显著的性能优势**。

  NIO并没有完全屏蔽平台差异，它仍然是基于各个操作系统的I/O系统实现的，差异仍然存在。使用NIO做网络编程构建事件驱动模型并不容易，陷阱重重。

  推荐大家使用成熟的NIO框架，如Netty，MINA等。解决了很多NIO的陷阱，并屏蔽了操作系统的差异，有较好的性能和编程模型。

# IO类型

* 概念
  * 用户空间与内核空间
    * 操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限
    * Linux操作系统将最高的1G字节供内核使用，将较低的3G字节，供各个进程使用
  * 进程切换
    * 内核必须有能力挂起在CPU上运行的进程，并恢复以前挂起的某个进程的执行，任何进程都是在操作系统内核的支持下运行的
    * 过程
      * 保存处理机上下文，包括程序计数器和其他寄存器
      * 更新PCB信息
      * 把进程的PCB移入相应的队列，如就绪，在某事件阻塞等队列
      * 选择另一个进程执行，并更新PCB
      * 更新内存管理的数据结构
      * 恢复处理机上下文
  * 进程阻塞
    * 是进程自身的一种主动行为，只有处于运行态的进程，才能将其转为阻塞状态。
    * 不占用CPU资源
  * 文件描述符fd
    * 表述指向文件的引用的抽象化概念
    * 一个非负整数，实际是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表
  * 缓存IO
    * 大多文件系统默认IO操作都是缓存IO
    * 操作系统会将IO的数据缓存在文件系统的页缓存中，数据会先被拷贝到内核缓冲区，再拷贝到应用程序的地址空间
    * 缺点
      * 传输过程中需要大应用程序地址为僮和内核进行多次数据拷贝操作，操作带来的CPU及内存开销非常大
  * 网络IO
    * 同步IO
    * BIO阻塞IO
    * 非阻塞IO
    * IO多路复用
    * 信号驱动IO
    * 异步IO

## 同步阻塞BIO

* 能够及时返回数据，无延迟
* 用户需付出性能的代价
* 常用类
  * InputStream
  * OutputStream
  * Reader
  * Writer

## 同步非阻塞NIO

* 轮询检查内核数据，直到数据准备好，再拷贝数据到进程(用户空间)，进行处理

* 优点
  * 能够在等待任务完成的时间里干其他活

* 缺点
  * 任务完成的响应延迟增大，因每过一段时间才轮询一次read操作，而任务可能在两次轮询之间的任意时间完成，整个数据吞吐量降低

### NIO与IO区别

  - IO是面向流，NIO是面向缓冲区
  - IO流阻塞，NIO流不阻塞
  - NIO有选择器，而IO没有

### NIO原理

* 核心组件
  * 通道`Channels`
    * Scatter: 从一个Channel读取的信息分散到N个缓冲区中(Buffer).
    * Gather: 将N个Buffer里面内容按照顺序发送到一个Channel.
    * 类型
      * `FileChannel`从文件读写数据 
      * `DataGramChannel`通过UDP读写网络数据
      * `SocketChannel`通过TCP
      * `ServerSocketChannel`可以监听新来的TCP连接,为每个连接建立一 个SocketChannel
  * 缓冲区`Buffers`
    * 缓冲区实质是一个数组(内存区)
    * 从Buffer把数据写入到Channels；从Channel中读取数据到buffers
    * Buffer本质上就是一块内存区；
    * 类型
      * `ByteBuffer`
      * `CharBuffer`
      * `ShortBuffer`
      * `IntBuffer`
      * `LongBuffer`
      * `FloatBuffer`
      * `DoubleBuffer`
    * 状态变量
      * `capacity`最大容量
      * `position`当前已读写字节数
      * `limit`还可以己度人读写字节数
  * 选择器`Selectors`
    * `Selector`选择器 / 多路复用器 
    * 一个线程用selector监听多个channel事件,实现IO多路复用
    * 避免了线程上下文切换带来的开销

## IO多路复用

* `select , poll, epoll`   (系统级别)
  * 单个process就可以同时处理多个网络连接的IO
  * 不断轮询所负责的所有socket，当某个socket有数据到达，就通知用户进程
* 实际中对于每一个socket，一般都设置成NIO，但是整个用户的process其实是一直被block的，不过process是被block这个函数block，而不是被socket IO给Block
* IO多路复用是阻塞在select，epoll这样的系统调用之上，而没有阻塞在真正的IO系统调用之上
* 通过把多个IO的阻塞复用到同一个select有阻塞上，从而使系统在单线程情况下，可以同时处理多个客户端请求

## 异步非阻塞AIO

* BIO在IO操作时开始阻塞应用程序，这意味着不可能同时重叠进行处理和IO操作
* NIO允许处理和IO操作重叠进行。但需要应用程序根据重现的规则来检查IO操作的状态
* AIO允许处理和IO操作重叠进行，包括IO操作完成的通知

# blocking与non-blocking区别

* blocling会一直block对应的进程直到操作完成
* non-blocking在kernel(内核)还准备数据的情况下立刻返回

# 同步IO与异步IO的区别

* 同步IO在 IO时会将process阻塞
  * BIO
  * NIO
    * 如果kernel数据没有准备好，这时不会block
    * 当kernel数据准备好的时候，将数据从内核拷贝到用户内存，为时进程被block
  * IO multiplexing
* 异步IO
  * 发起IO操作之后，直接返回，直到kernel发出信号，告诉进程IO完成



# IO多路复用

* 适用场合

  * 当处理多个描述符时
  * 一个客户处理多个套接口
  * 一个TCP服务器即要处理TCP，又要处理UDP
  
## select

* select 函数监视文件描述符，调用后select函数会阻塞，直到有描述符就绪，或者超时，函数返回

* 当select函数返回后，可通过遍历fdset，找到就绪的描述符

  * select本质是通过设置或检查存放fd标志位的数据结构来进行下一步处理

  * 优势

    * 跨平台

  * 缺点

    * **单进程能够监视的文件描述符的数量存在最大限制，Linux一般为1024**，可通过宏定义甚至重新编译内核的方式提升这一限制，但也会造成效率降低
    * 对socket扫描时是线性扫描，即采用轮询的方法，效率较低
      * 不管socket是否活跃，都遍历一遍，会浪费很好CPU时间
    * 需要维护一个存放大量fd的数据结构，这样使用户空间与内核空间在传递结构时复制开销大

## poll

* poll本质与select无区别,查询每个fd对应的设备状态,如果设备就绪则在设备等待队列中加入一项并继续遍历,如果遍历完所有fd后没有发现就绪设备,挂起当前进程,直到设备就緒或主动超时,被唤醒后它又要再次遍历fd,过程经历了多次无谓的遍历

  * 基于链表存储,没有最大连接数限制
  * 缺点
    * 大量fd数组被整体复制于用户态与内核之间,而不管是否有意义
    * 如果报告了fd后,没有被处理,下次poll会再次报告该fd(水平触发)
  * select与poll都需要在返回后,通过遍历文件描述符来获取已经就绪的socket

## epoll

**红黑树的数据结构**,使用一个fd管理多个fd,将用户关系的fd的事件放到内核的一个事件表中,用户空间和内核空间的copy只需一次

  * 原理
    * 支持水平触发和边缘触发(只告诉进程哪些fd刚刚变为就绪态,并且只通知一次)
    * epoll使用事件的就绪通知方式,一旦fd就绪,内核就会采用类似callback的回调机制来激活该fd
  * 优点
    * 没有最大并发连接的限制
    * 效率提升,不是轮询的方式,不会随着fd数目的增加效率下降,只有活跃的fd都会调用callback
    * 内存拷贝开销小
  * 文件描述符操作模式
    * LT(level trigger)
      * 检测到描述符事件发生并将此事件通知应用程序,应用程序可不处理,下次调用epoll_wait时,会再次响应应用程序并通知该事件
    * ET(edge trigger)
      * 检测到描述符事件发生并通知应用程序,应用程序立即处理,若不处理,下次不会通知

### [水平触发和边缘触发](https://blog.csdn.net/lihao21/article/details/67631516)

* 水平触发
* 边缘触发

## Select和epoll的区别

* **每次调用select,都要把fd(文件描述符)从用户态拷贝到内核态**,在fd很多时开销大
* 每次调用select都要在内核遍历传递进来的所有fd,在fd很多时开销时大
* select支持的文件供述符数量太小,默认为1024
* **epoll为每个fd指定一个回调函数,当设备准备就绪时,唤醒等待队列上的等待者时,就会调用这个回调函数**
* **epoll所支持的fd上限是最大可以打开文件数目**

# 字节流转与字符流

## [为何要有字符流](https://mp.weixin.qq.com/s/-DZj158-LOQmnCayf1_n3A)

**不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？**

* 字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道**编码类型就很容易出现乱码**问题。所以， I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。

## 将字节流转为字符流

* `InputStreamReader `类是从字节流到字符流的桥梁
  * 它读入字节，根据指定的编码方式，转为字符流    
* `OutputStreamWriter`类是从字符流转换成字节流

## 字符流与字符流区别

* 字节流操作时与文件本身直接操作，不会用到缓冲区。字符流会使用
* 字符流在操作文件时需使用`close()`方法，否则不输出内容
  * 说明其使用的是缓冲区，可使用flush()方法强制刷新缓冲区，此时才能在不close下输出
* Reader类的`read()`返回类型为int（0-65535）,若到流末尾，则返回-1
* inputStream类的`read()`也返回int，（0-255），若到末尾，则为-1
  * 汉字不能用0-255来表示。故只能用字符流
* 处理方式
  * 字节流处理字节和字节数组或二进制对象
  * 字符流处理字符，字符数组或字符串